# mcp_client_agent.py â€“ FastMCP 2.11.x & Ollama, robust + timeouts + retries
import asyncio, json, typing
import httpx

from fastmcp import Client

# ANSI color codes for terminal output
BLUE = "\033[94m"     # Bright blue for requests
GREEN = "\033[92m"    # Bright green for responses
RESET = "\033[0m"     # Reset color

# MCP server endpoint (streamable HTTP)
SERVER_URL = "http://127.0.0.1:8000/mcp/"

# Ollama chat endpoint
OLLAMA_URL = "http://127.0.0.1:11434/api/chat"


async def call_ollama(system_prompt: str, user_input: str, model: str) -> str:
    """Call Ollama /api/chat with a simple system+user exchange."""
    # Longer timeouts for CPU-only cold starts (Codespaces).
    timeout = httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=30.0)
    limits = httpx.Limits(max_keepalive_connections=5, max_connections=10)
    async with httpx.AsyncClient(timeout=timeout, limits=limits) as client:
        payload = {
            "model": model,
            # Treat the rendered prompt as the user message (pre-baked instruction + text)
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input},
            ],
            "stream": False,
        }

        # Retry on cold starts / transient failures
        attempts = 0
        last_exc = None
        while attempts < 4:
            try:
                resp = await client.post(OLLAMA_URL, json=payload)
                resp.raise_for_status()
                break
            except (httpx.TimeoutException, httpx.ConnectError) as e:
                last_exc = e
                attempts += 1
                await asyncio.sleep(1.5 * attempts)
                continue
            except httpx.HTTPStatusError as e:
                if 500 <= e.response.status_code < 600 and attempts < 3:
                    last_exc = e
                    attempts += 1
                    await asyncio.sleep(1.5 * attempts)
                    continue
                raise
        if attempts >= 4 and last_exc:
            raise last_exc

        data = resp.json()

        # Ollama native response: {"message": {"content": "..."}}
        if isinstance(data, dict) and "message" in data and isinstance(data["message"], dict):
            return data["message"].get("content", "") or ""

        # OpenAI-compatible fallback: {"choices": [{"message": {"content": "..."}}]}
        if isinstance(data, dict) and "choices" in data:
            choices = data["choices"]
            if choices and isinstance(choices[0], dict):
                msg = choices[0].get("message", {})
                if isinstance(msg, dict):
                    return msg.get("content", "") or ""

        return ""


async def main():
    async with Client(SERVER_URL) as client:
        # Discover prompts/tools (support wrapper objects OR raw lists)
        prompt_list = await client.list_prompts()
        tool_list = await client.list_tools()

        prompts = getattr(prompt_list, "prompts", prompt_list)
        tools = getattr(tool_list, "tools", tool_list)

        def _name(x):
            return getattr(x, "name", x.get("name") if isinstance(x, dict) else str(x))

        prompt_names = [_name(p) for p in prompts]
        tool_names = [_name(t) for t in tools]

        print(f"{BLUE}Available prompts: {', '.join(prompt_names)}{RESET}")
        print(f"{BLUE}Available tools: {', '.join(tool_names)}{RESET}\n")

        # Request model resource from server (robust to varying shapes)
        model_res = await client.read_resource("resource://model")

        def _as_text(content):
            if hasattr(content, "text"):
                return content.text
            if isinstance(content, dict) and "text" in content:
                return content["text"]
            if isinstance(content, str):
                return content
            return json.dumps(content)

        first = model_res[0] if isinstance(model_res, (list, tuple)) and model_res else model_res
        model_json = json.loads(_as_text(first))
        model_name = model_json.get("model", "llama3.2:latest")

        print(f"{BLUE}Model name used for Ollama: {model_name!r}{RESET}")

        # Optional one-time warm-up to avoid first interactive timeout
        try:
            print(f"{BLUE}Warming up Ollama model {model_name}...{RESET}")
            _ = await call_ollama(
                system_prompt="You are a helpful assistant.",
                user_input="Reply with 'ready'.",
                model=model_name,
            )
            print(f"{GREEN}Warm-up complete.{RESET}")
        except Exception as e:
            print(f"Warm-up skipped due to: {type(e).__name__}: {e!r}")

        while True:
            cmd = input("\nEnter tool name (or 'exit'): ").strip().lower()
            if cmd == "exit":
                break

            if cmd not in tool_names:
                print("Unknown tool:", cmd)
                continue

            user_input = input("Enter input text: ").strip()
            if not user_input:
                continue

            try:
                # Get the prompt template for this tool from the server
                print(f"{BLUE}Requesting prompt for tool: {cmd}{RESET}")
                # Provide a placeholder so server-side 'text' param (if required) is satisfied
                prompt_result = await client.get_prompt(cmd, arguments={"text": "{text}"})

                # Extract a 'user' message template robustly across shapes
                def _iter_messages(res):
                    msgs = getattr(res, "messages", None)
                    if msgs is None and isinstance(res, dict):
                        msgs = res.get("messages")
                    if msgs is None:
                        if isinstance(res, dict) and "role" in res and "content" in res:
                            msgs = [res]
                    return msgs or []

                def _first_text_from_content(content):
                    if isinstance(content, str):
                        return content
                    if hasattr(content, "text"):
                        return content.text
                    if isinstance(content, dict) and "text" in content:
                        return content["text"]
                    if isinstance(content, (list, tuple)):
                        for part in content:
                            if isinstance(part, str):
                                return part
                            if hasattr(part, "text"):
                                return part.text
                            if isinstance(part, dict) and "text" in part:
                                return part["text"]
                    return ""

                user_msg_template = ""
                for msg in _iter_messages(prompt_result):
                    role = getattr(msg, "role", msg.get("role") if isinstance(msg, dict) else None)
                    if role == "user":
                        content = getattr(msg, "content", msg.get("content") if isinstance(msg, dict) else None)
                        user_msg_template = _first_text_from_content(content)
                        if user_msg_template:
                           break

                if not user_msg_template:
                    user_msg_template = "You are a helpful assistant. Please operate on the following text:\n\n{text}"

                rendered_prompt = user_msg_template.format(text=user_input)

                print(f"{BLUE}Calling Ollama with user content: {rendered_prompt!r}{RESET}")
                result = await call_ollama(
                    system_prompt="You are a helpful assistant.",
                    user_input=rendered_prompt,
                    model=model_name,
                )

                # Display result in green
                print(f"\n{GREEN}[{cmd.upper()} RESULT]\n{result}{RESET}\n")

            except Exception as e:
                print(f"Unexpected error: {type(e).__name__}: {e!r}")


if __name__ == "__main__":
    asyncio.run(main())
