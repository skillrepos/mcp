# mcp_client_agent.py â€“ version for FastMCP 2.10.4 and Ollama
import asyncio
import httpx
import json
from fastmcp import Client

# ANSI color codes for terminal output
BLUE = "\033[94m"     # Bright blue for requests
GREEN = "\033[92m"    # Bright green for responses
RESET = "\033[0m"     # Reset color

# MCP server endpoint
SERVER_URL = "http://127.0.0.1:8000/mcp/"

# Ollama chat endpoint
OLLAMA_URL = "http://127.0.0.1:11434/api/chat"

async def call_ollama(system_prompt, user_input, model_name):
    """
    Send a chat message to the Ollama model with the given prompt and input.

    Args:
        system_prompt (str): System-level instruction for the model.
        user_input (str): User input to fill into the prompt.
        model_name (str): Ollama model to use.

    Returns:
        str: Response text from the model.
    """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    payload = {
        "model": model_name,
        "messages": messages,
        "stream": False
    }

    async with httpx.AsyncClient(timeout=60) as client:
        response = await client.post(OLLAMA_URL, json=payload)
        response.raise_for_status()
        result = response.json()

    # Return the best match from known response formats
    return (
        result.get("message", {}).get("content")
        or result.get("response")
        or str(result)
    ).strip()

async def main():
    # Create an async MCP client context
    async with Client(SERVER_URL) as client:
        print(f"{GREEN}Connected to MCP server.{RESET}")

        # Retrieve list of available tools from server
        tools = await client.list_tools()
        tool_names = [t.name for t in tools]
        print(f"{BLUE}Available tools: {', '.join(tool_names)}{RESET}\n")

        # Request model resource from server
        model_res = await client.read_resource("resource://model")
        model_json = json.loads(model_res[0].text)
        model_name_raw = model_json["model"]
        model_name = model_name_raw.split(":")[0]  # Normalize for Ollama

        print(f"{BLUE}Model name returned from server: {model_name_raw!r}{RESET}")
        print(f"{BLUE}Model name used for Ollama: {model_name!r}{RESET}")

        while True:
            cmd = input("\nEnter tool name (or 'exit'): ").strip().lower()
            if cmd == "exit":
                break

            if cmd not in tool_names:
                print("Unknown tool:", cmd)
                continue

            user_input = input("Enter input text: ").strip()
            if not user_input:
                continue

            try:
                # Get the prompt template for this tool from the server
                print(f"{BLUE}Requesting prompt for tool: {cmd}{RESET}")
                prompt_result = await client.get_prompt(cmd)

                # Extract user role template from the prompt
                user_msg_template = ""
                for msg in prompt_result.messages:
                    if msg.role == "user":
                        user_msg_template = msg.content.text
                        break

                # Fill in user input to generate final prompt
                rendered_prompt = user_msg_template.format(text=user_input)

                # Send the prompt and input to the Ollama model
                print(f"{BLUE}Calling Ollama with prompt: {rendered_prompt!r}{RESET}")
                result = await call_ollama(rendered_prompt, user_input, model_name)

                # Display result in green
                print(f"\n{GREEN}[{cmd.upper()} RESULT]\n{result}{RESET}\n")

            except Exception as e:
                print("Unexpected error:", e)

if __name__ == "__main__":
    asyncio.run(main())

